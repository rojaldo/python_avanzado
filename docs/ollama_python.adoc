:toc:
:toc-title: Índice
:source-highlighter: highlight.js

= Ollama Python

== Introduction

Ollama Python es una librería que permite interactuar con la API de Ollama desde Python. Funciona con Python 3.8 o superior.

== Instalación

.Para instalar Ollama Python, simplemente ejecute el siguiente comando:
[source,shell]
----
pip install ollama
----

== Casos de uso de Ollama Python

=== Funciones del cliente de Ollama

Ollama Python proporciona un cliente que permite interactuar con la API de Ollama. A continuación se muestran algunas de las funciones que se pueden realizar con el cliente de Ollama:
* Chat
* Generación de texto
* Listar modelos
* mostrar modelo
* Crear un modelo
* Descargar un modelo
* Copiar un modelo
* Eliminar un modelo
* Subir un modelo
* Obtener embeddings

.Un ejemplo de código con las funciones del cliente de Ollama se muestra a continuación:
[source,python]
----
import ollama

# Chat
ollama.chat(model='llama3.2', messages=[{'role': 'user', 'content': 'Why is the sky blue?'}])

# Generación de texto
ollama.generate_text(model='llama3.2', prompt='Once upon a time', max_tokens=50)

# Listar modelos
ollama.list()

# Mostrar modelo
ollama.show('llama3.2')

# Crear modelo
ollama.create(model='example', from_='llama3.2', system="You are Mario from Super Mario Bros.")

# Descargar un modelo
ollama.pull('llama3.2')

# Copiar un modelo
ollama.copy('llama3.2', 'llama3.2.backup')

# Eliminar un modelo
ollama.delete('llama3.2')

# Subir un modelo
ollama.push('user/llama3.2.backup')

# Obtener embeddings
ollama.embed(model='llama3.2', input='The sky is blue because of rayleigh scattering')
----

.Un ejemplo de código de chat:
[source,python]
----
import asyncio

from ollama import AsyncClient


async def main():
  messages = [
    {
      'role': 'user',
      'content': 'Why is the sky blue?',
    },
  ]

  client = AsyncClient()
  response = await client.chat('llama3.2', messages=messages)
  print(response['message']['content'])


if __name__ == '__main__':
  asyncio.run(main())
----

.El objeto `response` contiene la respuesta del modelo de lenguaje, que incluye el contenido del mensaje y otros metadatos.
[cols="2,6,1,2"]
|===
|Campo |Descripción |Tipo |Valor

|model
|Identifica el modelo de lenguaje utilizado para generar la respuesta
|String
|llama3.2

|created_at
|Marca de tiempo que indica cuándo se creó la respuesta
|DateTime
|2025-03-15T18:29:23.826310391Z

|done
|Indica si la generación de la respuesta se completó
|Boolean
|true

|done_reason
|Razón por la que se completó la generación (ej: stop, length, content_filter)
|String
|stop

|total_duration
|Tiempo total de procesamiento en nanosegundos
|Integer
|2347260137

|load_duration
|Tiempo empleado en cargar el modelo en nanosegundos
|Integer
|15150243

|prompt_eval_count
|Número de tokens evaluados en el prompt
|Integer
|31

|prompt_eval_duration
|Tiempo empleado en evaluar el prompt en nanosegundos
|Integer
|14000000

|eval_count
|Número total de tokens evaluados
|Integer
|319

|eval_duration
|Tiempo total de evaluación en nanosegundos
|Integer
|2316000000

|message.role
|Rol del emisor del mensaje (puede ser system, user, assistant)
|String
|assistant

|message.content
|Contenido textual de la respuesta generada
|String
|_Explicación sobre por qué el cielo es azul..._

|message.images
|Array de imágenes adjuntas a la respuesta (si las hubiera)
|Null
|null

|message.tool_calls
|Llamadas a herramientas realizadas durante la generación
|Null
|null
|===

.Ejemplo de respuesta estructurada en formato JSON:
[source,python]
----
import asyncio

from pydantic import BaseModel

from ollama import AsyncClient


# Define the schema for the response
class FriendInfo(BaseModel):
  name: str
  age: int
  is_available: bool


class FriendList(BaseModel):
  friends: list[FriendInfo]


async def main():
  client = AsyncClient()
  response = await client.chat(
    model='llama3.2',
    messages=[{'role': 'user', 'content': 'I have two friends. The first is Ollama 22 years old busy saving the world, and the second is Alonso 23 years old and wants to hang out. Return a list of friends in JSON format'}],
    format=FriendList.model_json_schema(),  # Use Pydantic to generate the schema
    options={'temperature': 0},  # Make responses more deterministic
  )

  # Use Pydantic to validate the response
  friends_response = FriendList.model_validate_json(response.message.content)
  print(friends_response)


if __name__ == '__main__':
  asyncio.run(main())
----

.Un ejemplo de llamadas a funciones de Ollama Python:
[source,python]
----
import asyncio

import ollama
from ollama import ChatResponse


def add_two_numbers(a: int, b: int) -> int:
  return a + b


def subtract_two_numbers(a: int, b: int) -> int:
  return a - b


# Tools can still be manually defined and passed into chat
subtract_two_numbers_tool = {
  'type': 'function',
  'function': {
    'name': 'subtract_two_numbers',
    'description': 'Subtract two numbers',
    'parameters': {
      'type': 'object',
      'required': ['a', 'b'],
      'properties': {
        'a': {'type': 'integer', 'description': 'The first number'},
        'b': {'type': 'integer', 'description': 'The second number'},
      },
    },
  },
}

messages = [{'role': 'user', 'content': 'What is three plus one?'}]
print('Prompt:', messages[0]['content'])

available_functions = {
  'add_two_numbers': add_two_numbers,
  'subtract_two_numbers': subtract_two_numbers,
}


async def main():
  client = ollama.AsyncClient()

  response: ChatResponse = await client.chat(
    'llama3.2',
    messages=messages,
    tools=[add_two_numbers, subtract_two_numbers_tool],
  )

  if response.message.tool_calls:
    # There may be multiple tool calls in the response
    for tool in response.message.tool_calls:
      # Ensure the function is available, and then call it
      if function_to_call := available_functions.get(tool.function.name):
        print('Calling function:', tool.function.name)
        print('Arguments:', tool.function.arguments)
        output = function_to_call(**tool.function.arguments)
        print('Function output:', output)
      else:
        print('Function', tool.function.name, 'not found')

  # Only needed to chat with the model using the tool call results
  if response.message.tool_calls:
    # Add the function response to messages for the model to use
    messages.append(response.message)
    messages.append({'role': 'tool', 'content': str(output), 'name': tool.function.name})

    # Get final response from model with function outputs
    final_response = await client.chat('llama3.2', messages=messages)
    print('Final response:', final_response.message.content)

  else:
    print('No tool calls returned from model')


if __name__ == '__main__':
  try:
    asyncio.run(main())
  except KeyboardInterrupt:
    print('\nGoodbye!')
----

.Histórico de chat:
[source,python]
----
from ollama import chat

messages = [
  {
    'role': 'user',
    'content': 'Why is the sky blue?',
  },
  {
    'role': 'assistant',
    'content': "The sky is blue because of the way the Earth's atmosphere scatters sunlight.",
  },
  {
    'role': 'user',
    'content': 'What is the weather in Tokyo?',
  },
  {
    'role': 'assistant',
    'content': 'The weather in Tokyo is typically warm and humid during the summer months.',
  },
]

while True:
  user_input = input('Chat with history: ')
  response = chat(
    'llama3.2',
    messages=messages
    + [
      {'role': 'user', 'content': user_input},
    ],
  )

  # Add the response to the messages to maintain the history
  messages += [
    {'role': 'user', 'content': user_input},
    {'role': 'assistant', 'content': response.message.content},
  ]
  print(response.message.content + '\n')
----

.Creación de un asistente personalizado:
[source,python]
----
from ollama import Client

client = Client()
response = client.create(
  model='my-assistant',
  from_='llama3.2',
  system='You are mario from Super Mario Bros.',
  stream=False,
  options={
    'temperature': 0.5,
    'max_tokens': 100,
    'top_p': 0.9,
    'top_k': 50
  },
)
print(response.status)
----

.El parámetro opcional _options_ en la función `generate` de la librería Python para Ollama es un diccionario que configura aspectos avanzados de la generación de texto. Dentro de este objeto, se pueden definir:
* **temperature**: Controla la creatividad o aleatoriedad de las respuestas. Cuanto mayor sea el valor, más creativas serán las respuestas.
* **max_tokens**: Establece el número máximo de tokens a generar (los tokens son unidades de texto en que se descomponen las palabras).
* **top_p**: Ajusta la fracción acumulada de probabilidad para la selección de palabras, limitando las alternativas a aquellas que suman un determinado umbral de probabilidad.
* **top_k**: Filtra las palabras más probables limitándolas a las *k* mejores opciones, excluyendo al resto.

.Ejemplo de modelo multimodal:
[source,python]
----
import httpx
from ollama import generate

raw = httpx.get('https://static.wikia.nocookie.net/villains/images/4/43/Bender.png')
raw.raise_for_status()

for response in generate('llava', 'explain this comic:', images=[raw.content], stream=True):
  print(response['response'], end='', flush=True)

print()
----
